{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE():\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, kld_weight=1.0, sess=None):\n",
    "        #self 54 [20,20] 2 kld_weight=10\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_encoder_dim = hidden_dims[0] #20\n",
    "        self.hidden_decoder_dim = hidden_dims[1] #20\n",
    "        self.latent_dim = latent_dim #2→μ,Σ\n",
    "        self.kld_weight = kld_weight\n",
    "        \n",
    "        self.construct_graph()\n",
    "        \n",
    "        if sess!=None:\n",
    "            self.sess = sess\n",
    "        else:\n",
    "            self.sess = tf.Session()\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def weight_variable(self, shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.001)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(self, shape):\n",
    "        initial = tf.constant(0., shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def construct_graph(self):\n",
    "        # データ用placeholder\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, self.input_dim])\n",
    "       \n",
    "        # 事前分布のパラメータ\n",
    "        self.mu_prior = tf.placeholder( tf.float32, shape=[None, self.latent_dim]) #average\n",
    "        self.logvar_prior = tf.placeholder( tf.float32, shape=[None, self.latent_dim]) #variance\n",
    "\n",
    "        # 入力→隠れ層 \n",
    "        W_encoder_input_hidden = self.weight_variable([self.input_dim,self.hidden_encoder_dim]) #54×20のガウス分布\n",
    "        b_encoder_input_hidden = self.bias_variable([self.hidden_encoder_dim]) #20の0\n",
    "\n",
    "        # 隠れ層の出力値\n",
    "        hidden_encoder = tf.nn.relu(tf.matmul(self.x, W_encoder_input_hidden) + b_encoder_input_hidden) #行列の掛け算のちバイアスのちrelu\n",
    "\n",
    "        # 隠れ層→潜在変数の平均と分散\n",
    "        W_encoder_hidden_mu = self.weight_variable([self.hidden_encoder_dim,self.latent_dim]) #20*2のガウス分布\n",
    "        b_encoder_hidden_mu = self.bias_variable([self.latent_dim]) #2の0\n",
    "        \n",
    "        W_encoder_hidden_logvar = self.weight_variable([self.hidden_encoder_dim,self.latent_dim]) #20*2 gaussian\n",
    "        b_encoder_hidden_logvar = self.bias_variable([self.latent_dim]) #2 no 0\n",
    "\n",
    "        # 平均と分散\n",
    "        mu_encoder = tf.matmul(hidden_encoder, W_encoder_hidden_mu) + b_encoder_hidden_mu #隠れ層の出力に重みかけてバイアス加算\n",
    "        logvar_encoder = tf.matmul(hidden_encoder, W_encoder_hidden_logvar) + b_encoder_hidden_logvar #隠れ層の出力に重みかけてバイアス加算\n",
    "\n",
    "        # 潜在変数のサンプリング\n",
    "        epsilon = tf.random_normal(tf.shape(logvar_encoder), name='epsilon')\n",
    "        std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "        self.z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "\n",
    "        # 潜在変数→隠れ層\n",
    "        W_decoder_z_hidden = self.weight_variable([self.latent_dim, self.hidden_decoder_dim])\n",
    "        b_decoder_z_hidden = self.bias_variable([self.hidden_decoder_dim])\n",
    "\n",
    "        # 隠れ層の値\n",
    "        hidden_decoder = tf.nn.relu(tf.matmul(self.z, W_decoder_z_hidden) + b_decoder_z_hidden)\n",
    "\n",
    "        # 隠れ層→出力層\n",
    "        W_decoder_hidden_reconstruction = self.weight_variable([self.hidden_decoder_dim, self.input_dim])\n",
    "        b_decoder_hidden_reconstruction = self.bias_variable([self.input_dim])\n",
    "\n",
    "        # 出力値\n",
    "        self.x_hat = tf.matmul(hidden_decoder, W_decoder_hidden_reconstruction) + b_decoder_hidden_reconstruction\n",
    "        \n",
    "        # 目的関数（KL距離とMSE）\n",
    "        #KLD = -0.5 * tf.reduce_sum(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "        KLD = -0.5 * tf.reduce_sum(1 + self.logvar_prior + logvar_encoder - (tf.pow(mu_encoder-self.mu_prior, 2) + tf.exp(logvar_encoder))/tf.exp(self.logvar_prior), reduction_indices=1)\n",
    "        \n",
    "        #BCE = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_hat, labels=x), reduction_indices=1)\n",
    "        MSE = tf.reduce_sum( tf.squared_difference(self.x_hat, self.x), reduction_indices=1)\n",
    "\n",
    "        #loss = tf.reduce_mean(100*BCE+KLD )\n",
    "        self.loss = tf.reduce_mean(MSE+KLD*self.kld_weight )\n",
    "        self.train_step = tf.train.AdamOptimizer(0.01).minimize(self.loss)\n",
    "        \n",
    "        # 再構成用のネットワーク(サンプリングをしない)\n",
    "        self.z_reconst = mu_encoder\n",
    "        hidden_decoder = tf.nn.relu(tf.matmul(self.z_reconst, W_decoder_z_hidden) + b_decoder_z_hidden)\n",
    "        self.x_reconst = tf.matmul(hidden_decoder, W_decoder_hidden_reconstruction) + b_decoder_hidden_reconstruction\n",
    "\n",
    "        \n",
    "    def train(self, x_data, n_steps, mu_prior, logvar_prior):\n",
    "        self.losses = []\n",
    "            \n",
    "        feed_dict = {self.x: x_data, self.logvar_prior: logvar_prior, self.mu_prior: mu_prior}\n",
    "        for step in range(1, n_steps):\n",
    "            _, cur_loss, self.z_, self.x_ = self.sess.run([self.train_step, self.loss, self.z, self.x_hat], feed_dict=feed_dict)\n",
    "            self.losses.append(cur_loss)\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(\"%03d - %lf\" % (step, cur_loss))\n",
    "                    \n",
    "\n",
    "        z, x = self.sess.run( [self.z_reconst, self.x_reconst], feed_dict=feed_dict )\n",
    "\n",
    "        # plot        \n",
    "        plt.title(\"latent variables\")\n",
    "        plt.plot( z[:,0], z[:,1], \"-o\" )\n",
    "\n",
    "        plt.figure()\n",
    "        plt.title(\"recostructed input\")\n",
    "        t = range(len(self.x_))\n",
    "        for d in range(self.input_dim):\n",
    "            plt.plot( t, x[:,d] )\n",
    "\n",
    "        plt.figure()\n",
    "        plt.title(\"loss\")\n",
    "        plt.plot( range(len(self.losses)), self.losses )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4c59e1b04f57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkld_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mgp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPTF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "x_data = np.loadtxt( \"000.txt\" )\n",
    "N = len(x_data)\n",
    "\n",
    "with tf.Session() as sess :\n",
    "    vae = VAE(len(x_data[0]), [20,20], 2, kld_weight=10, sess=sess)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    mu_prior = np.zeros( (N,2)  )\n",
    "    logvar_prior = np.zeros( (N,2) )\n",
    "    for it in range(50):\n",
    "        # VAE学習\n",
    "        vae.train( x_data, 100, mu_prior, logvar_prior )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
